\section{Logistic Regression Regression}

\subsection{Introduction}

In this section we present a remedial solution to combine our regression models with the pointwise-estimation baseline model, and a more principled solution. Since we are combining the models, our gold standards will now be the original annotated sets composed of with size between two to ten.

\subsection{Remedial Solution}

First we give brief reminder for our baseline model. We defined two possible events: $\pmb{\Omega} = \{s < t, s > t\},$ and after observing a sequence of comparisons between $s$ and $t$: $\pmb{S} = \{ s < t, s < t, \ldots, s > t \ldots \}$, we can ask what is the probability that the next element we will observe is $s < t$. This is a Bernoulli distribution with parameter $p$ and it is well known that the most likely $p$ is simply:

\begin{equation*}
	\Prob[ s < t ] = \frac{|\{ s < t \in \pmb{S} \}|}{|\pmb{S}|}.
\end{equation*}

In the baseline, if $\pmb{S}$ is empty then we defaulted to $\Prob[ s < t ] = \frac{1}{2}$. 

Now we present the remedial solution. Recall in the previous chapter we defined this probability value for the $\hat{y}$ output by elastic net regression:

\[   
\Prob[s < x ] = \left\{
\begin{array}{ll}
      \frac{1}{2} + \epsilon & \hat{y} < \delta \\
      \frac{1}{2} - \epsilon & otherwise,
\end{array} 
\right.
\]

while we used the actual probabilty value $p$ output by the logistic regression model. In the remedial solution, we use the elastic definition defined above, and in the case of logistic regression, we actually discard the value of $p$ and define:
\[   
\Prob[s < x ] = \left\{
\begin{array}{ll}
      \frac{1}{2} + \epsilon & p > \frac{1}{2} \\
      \frac{1}{2} - \epsilon & otherwise.
\end{array} 
\right.
\]

This captures our intuition that the prediction output by the model is less accruate than that of the actual data. Additionally, we also constructed a version of the Turk and Mohit's clusters where ties are removed. We reasoned that since our models are designed to predict ordering, while ties can be interepreted as synonyms, clusters generated without ties may give a more ``fair" representation of how well the models perform. Results are displayed below. 

\begin{table}
\small
\centering
\begin{tabular}{|l|cc|cc|cc|}
	% 
	\hline 
	& \multicolumn{2}{c|}{Elastic Net Regression } 
	& \multicolumn{2}{c|}{$l_1$-Logistic Regression} \\
	\hline 
	\bf Gold Set
	& \bf Pairwise & \bf Avg. $\tau$  
	& \bf Pairwise & \bf Avg. $\tau$  \\ 
	\hline
	% 
	BCS          & 90.0\% & 0.81 & 93.0\%  &  0.85 \\
	Turk         & 75.0\% & 0.62 & 74.0\%  &  0.61 \\
	Turk no-tie  & 81.0\% & 0.63 & 81.0\%  &  0.62 \\
	Mohit        & 74.0\% & 0.61 & 74.0\%  &  0.61 \\
	Mohit no-tie & 76.0\% & 0.52 & 76.0\%  &  0.53 \\
	% 
	\hline
\end{tabular}
\caption{\label{font-table}. Results for the two best models combined with pointwise estimation baseline in the remedial fashion. Note how two models performs comparable across all gold sets. In addition, not the gold clusters with no ties enjoyed a higher pairwise accuracy but suffer a lower $\tau$ value.}
\end{table}

\subsection{Solution with Beta Prior}

In this section we provide a more formal variant of the remedial solution. The heart of the of the problem is that we have some prior belief about the likelihood that one adjective is weaker than another, and an updated belief after observing some data, be it direct comparison or estimation from a model. Since we are modeling each edge a Bernoulli variable with parameter $\theta$ ranging over $[0,1]$, the prior is then a distribution over the Bernoulli $\theta$, this is the Beta distribution. In this next few paragraphs, we give a brief overview of Beta-Binomial model, in particular how it applies to our problem. 

It is well known that the prior the binomial and Bernoulli likelihood function is the Beta distribution with paramters $\beta_1, \beta_2 \in \{1, \ldots \}$, where we have:
	\begin{align*}
		\Prob[\theta | \beta_1, \beta_2 ] 
		&= \frac{\theta^{\beta_1 - 1} (1 - \theta)^{\beta_2 - 1}}{\int_{0}^1 \mu^{\beta_1 - 1} (1 - \mu)^{\beta_2 - 1} d \mu} \\
		&= \frac{\Gamma(\beta_1 + \beta_2)}{\Gamma(\beta_1)\Gamma(\beta_2)} \theta^{\beta_1 - 1} (1-\theta)^{\beta_2 - 1}.
	\end{align*}

The exact form of the $\Gamma$ function is beyond the scope of this introduction but the reader may select any introductory book on statistics for a refresher. 

Now after observing $n$ coin tosses with $h$ heads and $t$ tails for $h + t = n$, the posterior probability over $\theta$ given some prior setting of $\beta_1$ and $\beta_2$ is:
	\begin{align*}
		\Prob[\theta | h, t\beta_1, \beta_2 ] 
		&= \frac{\Prob[ h | n, \theta] \Prob[\theta |, n, \beta_1, \beta_2]}{\Prob[h|n, \beta_1 + \beta_2]} \\
		&\propto \theta^{h + \beta_1 - 1} (1 - \theta)^{t + \beta_2 - 1},
	\end{align*}

note the posterior distribution is also a beta distribution. Now we a have the distribution $\Prob[\theta | h, t\beta_1, \beta_2 ]$, we can return the the pointwise estimation setting and ask what is the likelihood the next toss lands heads, this is exactly the posteriror mean:
	\begin{align*}
		\pmb{E}[\theta | h, t\beta_1, \beta_2 ]  
		&= \int_{0}^1 \theta \Prob[\theta | h, t\beta_1, \beta_2 ] d \theta \\
		&= \frac{\beta_1 + h}{\beta_1 + \beta_2 + n}.
	\end{align*}

Not how the last line appeals strongly to intuition and therefore can be easily used: the expected outcome of the next toss given the prior is simply the prior tosses plus the tosses observed from data. 

In our setting, we fix the ratio of $\beta_1$ and $\beta_2$ so that the prior probability is exactly $1/2$, thus reflecting our ignorance. Note this is consistent with our ad-hoc setting in the remedial solution. The exactly values of $\beta_1$ and $\beta_2$ is a hyperparameter to be tuned, in practice we set $\beta_1 = \beta_2 = 1$. The coin tosses observed from data and the model are also hyperparameters. Note the more confident we are with the data, the larger the values of $h$ and $t$ should be with respect to $\beta_1$ and $\beta_2$. We experimented with a variety of values, and settled on the following settings for $h$ and $t$:
	\begin{enumerate}
		\item If there is an observation, then we use the raw comparison counts between the adjectives as $h$ and $t$
		\item If there is no obervation so we are using the model, we set $h$ to be the probability that the model predicts less than, and $t = 1 - h$.
	\end{enumerate}

In informal terms, we are confident in the quality of direct comparisons, if they can be observed, and not very confident in the prediction of the model. Results for Beta-Binomal model is presented below.  All in all, the best model uses the beta-binomial model to combine direct observations with $l-1$-penalized logistic regression model, the regression model uses top the coin toss probability of 10 most connect neighbors as features. This model achieved $75\%$ pairwise accuracy on Mohit's data set and the Turk set, and a Kendall's $\tau$ value of $0.61$ and $0.62$. After adjusting for ties, the pairwise accuracy on Mohit's data set was $76\%$, which approaches the inter-annotator accuracy of $78\%$, while the pairwise accuracy on the Turks set was $82\%$ after adjusting for ties. 

On the other hand, we observe that although Bansal's method performs well on the gold set they procured, it performs substantially worse across all other data sets: the $\tau$'s are close to zero and the pairwise accuracy is comparably low. Such surprisingly low results warrants further inspection. The low score actually hides two sources of error: error from absolute no data for any pair of adjectives, and error from wrong rankings when there is some data. In order to disentangle the two sources of error, we constructed a subset of the Turk set and the base-comparative-superlative sets where at least one pair of words from each cluster must have some data. Recall the original Turk set has $79$ clusters, the new set only has 23 clusters, that is to say less than $30\%$ of the Turk clusters appear in the N-gram data set. The story is similarly bleak for the base-comparative-superlative gold set: only 64 out of 238 clusters has any data at all, this is just $27\%$. After adjusting for data sparsity, we see that that Bansal's method achieves $65\%$ accuracy on the Turk set, which is more in line with how it performed on the Mohit gold set. On the base-comparative-superlative set, Mohit's method only achieved $56\%$ accuracy, this is not as high as the other gold set, but certainly better than random. We also report results for our regression method using N-gram and PPDB data for the two reduced gold sets, and observe the results are comparable two their performance on the full gold set as expected. All in all, it is clear that the increase in performance is primarily due to better coverage due to PPDB data. 

\begin{table}
\small
\centering
\begin{tabular}{|l|cc|cc|cc|}
	% 
	\hline 
	& \multicolumn{2}{c|}{Elastic Net Regression } 
	& \multicolumn{2}{c|}{$l_1$-Logistic Regression} 
	& \multicolumn{2}{c|}{MILP} \\
	\hline 
	\bf Gold Set
	& \bf Pairwise & \bf Avg. $\tau$  
	& \bf Pairwise & \bf Avg. $\tau$  
	& \bf Pairwise & \bf Avg. $\tau$  \\ 
	\hline
	% 
	BCS          & 90.0\% & 0.81 & \pmb{93.0\%}  &  \pmb{0.85} & 18.0\%  &  0.02 \\
	Turk         & 75.0\% & 0.62 & \pmb{75.0\%}  &  \pmb{0.62} & 25.0\%  &  0.13 \\
	Mohit        & 74.0\% & 0.61 & \pmb{75.0\%}  &  \pmb{0.61} & 69.6\%  &  0.57 \\
	Turk no-tie  & 81.0\% & 0.63 & \pmb{82.0\%}  &  \pmb{0.63} & 19.0\%  &  0.12 \\
	Mohit no-tie & 76.0\% & 0.52 & \pmb{76.0\%}  &  \pmb{0.53} & 68.0\%  &  0.46 \\
	% 
	\hline
	% 
	BCS has-data  & 100.0\% & 1.00 & 100.0\%  & 1.00 & 56.0\%  & 0.13 \\
	Turk has-data & 66.0\%  & 0.47 & 66.0 \%  & 0.47 & 65.0\%  & 0.46 \\
	% 
	\hline
\end{tabular}
\caption{\label{font-table}. The first two columns show results for the two best models combined with pointwise estimation baseline using Beta-Binomial model. The third column displays Mohit's MILP method using N-gram data only. The results shows that $l_1$-logistic regression outperformed elastic net regression on most data sets by a small (possibly insignificant) margin, otherwise they are equivalent. In particular, oberve how logistic regression performs just as well on Mohit's set as it does no the Turk set. Furthermore, both model outperform MILP by a non-trivial amount on all gold sets. Finally, note how well the MILP method performs on Mohit's gold cluster, versus how poorly it performs on other gold standards. The bottom two rows report results across all methods for the subsets of the gold clusters where there is some N-gram data. }
\end{table}\newpage

\subsection{Conclusion and Future Work}

We will close this chapter by reiterating our assumption, and offer some possible avenues of exploration. First, we review the fundamental hypothesis underlying all methods we have seen so far: there is a positive relationship between how words are used on average in the context of intensifiers such as adverbs or N-gram patterns, and how an annotator ranks pairs of such words in isolation. The fact that all successful methods we have seen so far achieved close to or above $70\%$ pairwise accuracy confirms this hypothesis. However, we must caution those who wish to improve upon this results, inter-annotator accuracy on Mohit's set is $78\%$. In fact, many of the rankings are subjective and we do not believe it is meaningful to achieve, for example $90\%$, on the gold standards procured by Mohit and us. 

In fact, consider this cluster: 

\begin{itemize}
\item interesting
\item entertaining
\item fascinating
\item intriguing
\item amusing
\item exciting,
\end{itemize}

the gold set suggests
	\[
		\text{interesting} < \text{intriguing} < \text{amusing} = \text{entertaining} < \text{fascinating} = \text{exciting},
	\]

but an equally plausible ranking is:

	\[
		\text{interesting} < \text{amusing} < \text{entertaining} < \text{fascinating} < \text{intriguing} < \text{exciting}.
	\]

Penalized elastic net regression output:

	\[
		\text{interesting} < \text{entertaining} < \text{fascinating} < \text{intriguing} < \text{amusing} < \text{exciting}.
	\]

We leave it to the reader to judge which cluster makes the most sense, if any. This example suggests an immediate improvement upon our work: procure better clusters so the results are more meaningful. There are two ways to improve the quality of each cluster: prune the clusters for synonyms, and resist the temptation for large clusters. The first suggestion is self explanatory, since none of the models account for synonyms, clusters that contain words annotators would consider to be synonyms will unfairly depress the score. Eschewing large clusters is important because the larger the cluster, the more likely polysemy will 
























































